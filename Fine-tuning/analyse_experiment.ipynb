{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b2266ff",
   "metadata": {},
   "source": [
    "# ASR Data Augmentation ‚Äî Analyse Results \n",
    "\n",
    "This notebook provides a **reproducible framework** to evaluate, analyze, and compare Automatic Speech Recognition (ASR) models on **pathological voices**.  \n",
    "It is designed to accompany the thesis *‚ÄúImproving Automatic Speech Recognition for Pathological\n",
    "Voices Using Data Augmentation‚Äù* and allows running a baseline evaluation and analyzing custom experiments.\n",
    "\n",
    "### Goals\n",
    "- Establish a **baseline WER** for pathological speech using a pretrained ASR model.  \n",
    "- Analyze and compare different augmentation and fine-tuning experiments.  \n",
    "- Provide visual and quantitative summaries of model performance.  \n",
    "\n",
    "### Input\n",
    "- Collected pathological dataset (used for baseline evaluation).  \n",
    "- `metrics.csv` files from training runs (NeMo logs).  \n",
    "\n",
    "### Process\n",
    "1. **Baseline Evaluation**  \n",
    "   - Compute Word Error Rate (WER) of a pretrained NeMo ASR model (e.g., `stt_it_conformer_ctc_large`).  \n",
    "   - Use results as a reference point before applying augmentation or fine-tuning.  \n",
    "\n",
    "2. **Experiment Analysis**  \n",
    "   - Load experiment logs (`metrics.csv`).  \n",
    "   - Plot learning curves (WER vs. epoch) with confidence intervals.  \n",
    "   - Compare multiple experiment groups side by side.  \n",
    "   - Summarize best performance (lowest mean WER, confidence intervals, relative improvements).  \n",
    "   - Ensure that recognition accuracy for healthy speech does not degrade.  \n",
    "   - Visualize and compare multiple runs against the baseline.  \n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd86191",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "from scipy import stats\n",
    "import torch\n",
    "from jiwer import wer, Compose, ToLowerCase, RemovePunctuation, RemoveMultipleSpaces, Strip\n",
    "from nemo.collections.asr.models import EncDecCTCModel, EncDecCTCModelBPE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79cd655",
   "metadata": {},
   "source": [
    "## CONFIG ‚Äî set your paths and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5335fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =======================================\n",
    "# Configs for the experiment analysis\n",
    "# =======================================\n",
    "\n",
    "# Root directory of your trainer outputs (matches your config.yaml paths.output_dir)\n",
    "OUTPUT_DIR = Path(\"./nemo_out\") \n",
    "\n",
    "# Path to your metrics CSV files (relative to OUTPUT_DIR)\n",
    "METRICS_PATTERN = \"logs/job_*/lightning_logs/**/metrics.csv\"\n",
    "\n",
    "# Columns in metrics.csv\n",
    "EPOCH_COL   = \"epoch\"\n",
    "VAL_WER_COL = \"collected_pathological_eval_val_wer\"\n",
    "\n",
    "\n",
    "# ================================================\n",
    "# Configs for the Baseline (NeMo pretrained model)\n",
    "# ================================================\n",
    "\n",
    "CSV_PATH   = Path(\"/data/speech-project/datasets/collected_dataset/dataset.csv\") \n",
    "    # The dataset CSV (CSV_PATH) must contain at least these columns:\n",
    "    #   - \"is_health\"      : marks whether the utterance is healthy (t/true/1) or pathological (f/false/0)\n",
    "    #   - \"text\"           : the ground-truth transcript of the utterance\n",
    "    #   - \"filename_path\"  : relative path to the audio file, resolved against AUDIO_ROOT\n",
    "\n",
    "# folder with the audio files\n",
    "AUDIO_ROOT = Path(\"/data/speech-project/datasets/collected_dataset\")   \n",
    "\n",
    "MODEL_NAME = \"stt_it_conformer_ctc_large\"  \n",
    "\n",
    "BASELINE = 0.457\n",
    "    # If you have already computed the baseline WER, set it here to avoid recomputation\n",
    "    # Otherwise, you can compute it in the next cell \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f3a068",
   "metadata": {},
   "source": [
    "## Optional - Compute the Baseline for Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcd5e81",
   "metadata": {},
   "source": [
    "### üîé Baseline Evaluation (Pretrained NeMo Model, No Fine-Tuning)\n",
    "\n",
    "This step computes the **baseline Word Error Rate (WER)** of a pretrained ASR model on your *pathological speech dataset*.  \n",
    "\n",
    "**What happens here:**\n",
    "1. Load your dataset metadata (`dataset.csv`), which must contain at least:\n",
    "   - `is_health` ‚Üí True/False flag for healthy speech  \n",
    "   - `text` ‚Üí Reference transcript of the utterance  \n",
    "   - `filename_path` ‚Üí Relative path to the audio file  \n",
    "2. Filter for pathological recordings only (`is_health = False`).  \n",
    "3. Load the corresponding audio files from `AUDIO_ROOT`.  \n",
    "4. Run the pretrained ASR model (without fine-tuning) to generate transcriptions.  \n",
    "5. Compute the WER against the reference transcripts.  \n",
    "6. Print the baseline WER and show sample predictions.  \n",
    "\n",
    "üëâ This establishes a **reference performance** before applying fine-tuning, augmentation, or synthesized data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070fa56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_normalize = Compose([ToLowerCase(), RemovePunctuation(), RemoveMultipleSpaces(), Strip()])\n",
    "\n",
    "def baseline_pathological_wer(\n",
    "    csv_path: str | Path = CSV_PATH,\n",
    "    audio_root: str | Path = AUDIO_ROOT,\n",
    "    model_name: str = MODEL_NAME,\n",
    "    batch_size: int = 4,      \n",
    "    num_workers: int = 0,   \n",
    "    preview_k: int = 10,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Compute baseline WER on pathological utterances using a pretrained (NOT fine-tuned) NeMo CTC model.\n",
    "    Equivalent to the flat script you shared, but wrapped in a function.\n",
    "    \"\"\"\n",
    "    # ==== LOAD DATA ====\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    def _is_pathological(v):\n",
    "        if pd.isna(v): \n",
    "            return False\n",
    "        s = str(v).strip().lower()\n",
    "        return s in {\"f\", \"false\", \"0\", \"no\"}\n",
    "\n",
    "    df = df[df[\"is_health\"].apply(_is_pathological)].copy()\n",
    "\n",
    "    if \"audio_filepath\" in df.columns:\n",
    "        audio_paths = df[\"audio_filepath\"].astype(str).tolist()\n",
    "    elif \"filename_path\" in df.columns:\n",
    "        audio_paths = [str(Path(audio_root) / p) for p in df[\"filename_path\"].astype(str)]\n",
    "    else:\n",
    "        raise ValueError(\"CSV must include either 'audio_filepath' or 'filename_path'.\")\n",
    "\n",
    "    refs = [_normalize(t) for t in df[\"text\"].astype(str).tolist()]\n",
    "    print(f\"Evaluating {len(audio_paths)} pathological utterances.\")\n",
    "\n",
    "    # ==== LOAD MODEL ====\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = EncDecCTCModelBPE.from_pretrained(model_name, map_location=device)\n",
    "    model.eval()\n",
    "\n",
    "    # ==== TRANSCRIBE ====\n",
    "    hyp_objects = model.transcribe(audio_paths, batch_size=batch_size, num_workers=num_workers)\n",
    "    hyps = [_normalize(h.text) if hasattr(h, \"text\") else _normalize(h) for h in hyp_objects]\n",
    "\n",
    "    # ==== COMPUTE WER ====\n",
    "    baseline = wer(refs, hyps)\n",
    "    print(f\"\\nüîé Baseline WER (pretrained {model_name}, pathological only): {baseline:.4f}\\n\")\n",
    "\n",
    "    # ==== SHOW PREDICTIONS ====\n",
    "    print(\"üóí Sample Predictions:\\n\")\n",
    "    for i in range(min(preview_k, len(refs))):\n",
    "        print(f\"[{i+1}]\")\n",
    "        print(f\"üó£ REF: {refs[i]}\")\n",
    "        print(f\"ü§ñ HYP: {hyps[i]}\")\n",
    "        print()\n",
    "\n",
    "    return baseline\n",
    "baseline_pathological_wer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0493742c",
   "metadata": {},
   "source": [
    "## Helper Functions to Analyze Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc739241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- discovery ---------------------------------------------------------------\n",
    "def debug_list_job(job_id, output_dir=OUTPUT_DIR, pattern=METRICS_PATTERN):\n",
    "    pat = pattern.replace(\"job_*\", f\"job_{str(job_id).strip()}\")\n",
    "    found = sorted(output_dir.glob(pat))\n",
    "    print(f\"job_{job_id}: {len(found)} file(s)\")\n",
    "    for p in found[:10]:\n",
    "        print(\" -\", p)\n",
    "    return found\n",
    "\n",
    "def job_paths_from_ids(*job_ids, output_dir=OUTPUT_DIR, pattern=METRICS_PATTERN):\n",
    "    \"\"\"Return list of metrics.csv paths for the given job IDs.\"\"\"\n",
    "    out = []\n",
    "    for j in job_ids:\n",
    "        out += debug_list_job(j, output_dir=output_dir, pattern=pattern)\n",
    "    return out\n",
    "\n",
    "# -- 1) Per-run split plot (your style) -------------------------------------\n",
    "def plot_result(csv_path, title=''):\n",
    "    \"\"\"Plot pathological/healthy/synth WER curves for a single metrics.csv (display only).\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Pathological\n",
    "    if \"collected_pathological_eval_val_wer\" in df.columns:\n",
    "        tmp = df[['epoch', 'collected_pathological_eval_val_wer']].dropna()\n",
    "        plt.plot(tmp['epoch'], tmp['collected_pathological_eval_val_wer'], marker='o', label='Collected Pathological WER')\n",
    "\n",
    "    # Healthy\n",
    "    if \"collected_healthy_eval_val_wer\" in df.columns:\n",
    "        tmp = df[['epoch', 'collected_healthy_eval_val_wer']].dropna()\n",
    "        plt.plot(tmp['epoch'], tmp['collected_healthy_eval_val_wer'], marker='o', label='Collected Healthy WER')\n",
    "\n",
    "    # Synth\n",
    "    if \"synth_eval_val_wer\" in df.columns:\n",
    "        tmp = df[['epoch', 'synth_eval_val_wer']].dropna()\n",
    "        plt.plot(tmp['epoch'], tmp['synth_eval_val_wer'], marker='o', linestyle='--', label='Synthesized Eval WER')\n",
    "\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Validation WER')\n",
    "    plt.title(title or str(csv_path))\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print lowest pathological WER if present\n",
    "    col = 'collected_pathological_eval_val_wer'\n",
    "    if col in df.columns:\n",
    "        tmp = df[['epoch', col]].dropna()\n",
    "        if not tmp.empty:\n",
    "            min_wer = tmp[col].min()\n",
    "            min_epoch = int(tmp.loc[tmp[col].idxmin(), 'epoch'])\n",
    "            print(f\"Lowest Pathological WER: {min_wer:.4f} at epoch {min_epoch}\")\n",
    "\n",
    "# -- 2) Multi-run overlay of pathological split ------------------------------\n",
    "def plot_pathological_wer_multi(csv_paths, title='Collected Pathological Eval WER'):\n",
    "    \"\"\"Each run as a separate line; legend shows job_<ID>.\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plotted = 0\n",
    "    for path in csv_paths:\n",
    "        df = pd.read_csv(path)\n",
    "        col = \"collected_pathological_eval_val_wer\"\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        tmp = df[['epoch', col]].dropna()\n",
    "        # derive label like \"job_52355\"\n",
    "        parts = Path(path).parts\n",
    "        lab = next((p for p in parts if p.startswith(\"job_\")), Path(path).name)\n",
    "        plt.plot(tmp['epoch'], tmp[col], marker='o', label=lab)\n",
    "        plotted += 1\n",
    "\n",
    "    if plotted == 0:\n",
    "        print(\"[WARN] None of the CSVs had 'collected_pathological_eval_val_wer'.\")\n",
    "        return\n",
    "\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Validation WER on Real Pathological Speech')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# -- 3) Best WER per file (pathological) -------------------------------------\n",
    "def lowest_pathological_wer(csv_paths):\n",
    "    \"\"\"Print min pathological WER per file (display only).\"\"\"\n",
    "    col = \"collected_pathological_eval_val_wer\"\n",
    "    any_printed = False\n",
    "    for path in csv_paths:\n",
    "        df = pd.read_csv(path)\n",
    "        if col not in df.columns: \n",
    "            continue\n",
    "        tmp = df[['epoch', col]].dropna()\n",
    "        if tmp.empty: \n",
    "            continue\n",
    "        min_wer = tmp[col].min()\n",
    "        min_epoch = int(tmp.loc[tmp[col].idxmin(), 'epoch'])\n",
    "        print(f\"{path}: Lowest Pathological WER: {min_wer:.4f} at epoch {min_epoch}\")\n",
    "        any_printed = True\n",
    "    if not any_printed:\n",
    "        print(\"[INFO] No pathological WER column found in provided CSVs.\")\n",
    "\n",
    "# -- 4) Healthy WER monotonic check ------------------------------------------\n",
    "def healthy_never_rises_multi(csv_paths, tolerance=1e-4, verbose=True):\n",
    "    \"\"\"\n",
    "    Check that collected healthy validation WER never increases (beyond tolerance).\n",
    "    Returns {csv_path: bool}\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for path in csv_paths:\n",
    "        df = pd.read_csv(path)\n",
    "        if \"collected_healthy_eval_val_wer\" in df.columns:\n",
    "            hcol = \"collected_healthy_eval_val_wer\"\n",
    "        elif \"healthy_val_wer\" in df.columns:\n",
    "            hcol = \"healthy_val_wer\"\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(f\"[WARN] No healthy WER column in {path}\")\n",
    "            continue\n",
    "\n",
    "        x = df[['epoch', hcol]].dropna().sort_values('epoch').reset_index(drop=True)\n",
    "        running_min = x[hcol].cummin()\n",
    "        prev_min = running_min.shift(1).fillna(x[hcol].iloc[0])\n",
    "        rise = x[hcol] - prev_min\n",
    "        ok = not (rise > tolerance).any()\n",
    "        results[path] = ok\n",
    "\n",
    "        if verbose:\n",
    "            if ok:\n",
    "                print(f\"‚úÖ {path}: PASS (never rises, tol={tolerance})\")\n",
    "            else:\n",
    "                first = x.loc[(rise > tolerance)].iloc[0]\n",
    "                print(f\"‚ùå {path}: FAIL at epoch {int(first['epoch'])} (rise {float(rise.loc[first.name]):.6f})\")\n",
    "    return results\n",
    "\n",
    "# -- 5) Grouped mean ¬± 95% CI for ONE experiment (pathological) --------------\n",
    "def analyze_single_experiment(\n",
    "    csv_paths,\n",
    "    label='Experiment',\n",
    "    max_epoch=None,\n",
    "    show_plot=True,\n",
    "    starting_wer=BASELINE,                           \n",
    "    wer_col='collected_pathological_eval_val_wer',\n",
    "    min_runs_per_epoch=1,\n",
    "    show_baseline=True,\n",
    "    show_ci=True,\n",
    "    color=None\n",
    "):\n",
    "    \"\"\"Aggregate repeated runs (same experiment) ‚Üí mean ¬± 95% CI over epochs (display only).\"\"\"\n",
    "    runs = []\n",
    "    for i, path in enumerate(csv_paths):\n",
    "        df = pd.read_csv(path)\n",
    "        if wer_col not in df.columns:\n",
    "            continue\n",
    "        d = df.dropna(subset=['epoch', wer_col])\n",
    "        if max_epoch is not None:\n",
    "            d = d[d['epoch'] <= max_epoch]\n",
    "        d = d[['epoch', wer_col]].copy()\n",
    "        d['run_id'] = i\n",
    "        runs.append(d)\n",
    "    if not runs:\n",
    "        raise ValueError(\"No valid CSVs/columns found for this experiment.\")\n",
    "\n",
    "    combined = pd.concat(runs, ignore_index=True)\n",
    "\n",
    "    if show_baseline:\n",
    "        n_runs = combined['run_id'].nunique()\n",
    "        baselines = pd.DataFrame({'epoch':[0]*n_runs, wer_col:[starting_wer]*n_runs, 'run_id':list(range(n_runs))})\n",
    "        combined = pd.concat([baselines, combined], ignore_index=True)\n",
    "\n",
    "    summary = (\n",
    "        combined.groupby('epoch')[wer_col]\n",
    "        .agg(['mean','std','count'])\n",
    "        .reset_index()\n",
    "        .sort_values('epoch')\n",
    "    )\n",
    "    summary = summary[summary['count'] >= min_runs_per_epoch]\n",
    "    summary['sem']  = summary['std'] / np.sqrt(summary['count'].clip(lower=1))\n",
    "    t_quant         = stats.t.ppf(0.975, np.maximum(summary['count'] - 1, 1))\n",
    "    summary['ci95'] = summary['sem'].fillna(0) * t_quant\n",
    "\n",
    "    if show_plot:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        x = summary['epoch'].values\n",
    "        y = summary['mean'].values\n",
    "        e = summary['ci95'].values\n",
    "        plt.plot(x, y, linewidth=2.5, label=f'{label} (mean)', color=color)\n",
    "        if show_ci:\n",
    "            plt.fill_between(x, y - e, y + e, alpha=0.20, linewidth=0, label=f'{label} 95% CI', color=color)\n",
    "        if show_baseline:\n",
    "            plt.axhline(starting_wer, linestyle=':', linewidth=1.2, alpha=0.8, label=f'Baseline {starting_wer:.0%}')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Validation WER on Real Pathological Speech')\n",
    "        plt.gca().yaxis.set_major_formatter(PercentFormatter(xmax=1.0, decimals=0))\n",
    "        plt.grid(True, alpha=0.25)\n",
    "        plt.legend(frameon=False, loc='upper right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Text summary\n",
    "    min_row = summary.loc[summary['mean'].idxmin()]\n",
    "    min_epoch = int(min_row['epoch'])\n",
    "    min_mean_wer = float(min_row['mean'])\n",
    "    abs_impr = starting_wer - min_mean_wer\n",
    "    rel_impr = (abs_impr / starting_wer) * 100\n",
    "    print(\"üìä Summary\")\n",
    "    print(\"----------\")\n",
    "    print(f\"üîΩ Best mean WER: {min_mean_wer:.4f} at epoch {min_epoch}\")\n",
    "    print(f\"‚úÖ Absolute improvement from {starting_wer:.2f}: {abs_impr:.4f}\")\n",
    "    print(f\"üìà Relative improvement: {rel_impr:.2f}%\")\n",
    "\n",
    "    return summary\n",
    "\n",
    "# -- 6) Multi-experiment comparison (pathological, display only) --------------\n",
    "def analyze_experiments_grouped(\n",
    "    group_paths_dict,            # {\"Exp A\": [...paths...], \"Exp B\": [...paths...] }\n",
    "    max_epoch=None,\n",
    "    show_plot=True,\n",
    "    starting_wer=None,\n",
    "    show_shadow=True,\n",
    "    color_map=None,              # Optional: {\"Exp A\": \"red\", \"Exp B\": \"blue\"}\n",
    "    wer_col=None                 # Specify which column to compare\n",
    "):\n",
    "    \"\"\"\n",
    "    Overlay experiments: mean ¬± 95% CI per epoch (display only).\n",
    "    wer_col: column name to compare (default: VAL_WER_COL)\n",
    "    starting_wer: baseline for improvement\n",
    "    \"\"\"\n",
    "    if wer_col is None:\n",
    "        wer_col = VAL_WER_COL\n",
    "    if starting_wer is None:\n",
    "        starting_wer = BASELINE\n",
    "\n",
    "    all_data = []\n",
    "\n",
    "    # collect runs\n",
    "    for label, paths in group_paths_dict.items():\n",
    "        for rid, path in enumerate(paths):\n",
    "            df = pd.read_csv(path)\n",
    "            if wer_col not in df.columns:\n",
    "                continue\n",
    "            d = df.dropna(subset=[EPOCH_COL, wer_col])\n",
    "            if max_epoch is not None:\n",
    "                d = d[d[EPOCH_COL] <= max_epoch]\n",
    "            d = d[[EPOCH_COL, wer_col]].copy()\n",
    "            d['run_id'] = rid\n",
    "            d['label'] = label\n",
    "            all_data.append(d)\n",
    "\n",
    "    if not all_data:\n",
    "        raise ValueError(\"No valid CSVs found for any group.\")\n",
    "\n",
    "    df_all = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "    # summary stats\n",
    "    summary = (\n",
    "        df_all.groupby(['label', EPOCH_COL])[wer_col]\n",
    "        .agg(['mean', 'std', 'count'])\n",
    "        .reset_index()\n",
    "        .sort_values(['label', EPOCH_COL])\n",
    "    )\n",
    "    summary['sem']  = summary['std'] / np.sqrt(summary['count'].clip(lower=1))\n",
    "    t_quant         = stats.t.ppf(0.975, np.maximum(summary['count'] - 1, 1))\n",
    "    summary['ci95'] = summary['sem'].fillna(0) * t_quant\n",
    "\n",
    "    # plotting\n",
    "    if show_plot:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        ax = plt.gca()\n",
    "        for label in summary['label'].unique():\n",
    "            g = summary[summary['label'] == label]\n",
    "            x = g[EPOCH_COL].values\n",
    "            y = g['mean'].values\n",
    "            e = g['ci95'].values\n",
    "            color = color_map[label] if color_map and label in color_map else None\n",
    "            (ln,) = ax.plot(x, y, linewidth=2, label=label, color=color)\n",
    "            if show_shadow:\n",
    "                ax.fill_between(x, y - e, y + e, alpha=0.20, linewidth=0, color=ln.get_color())\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel(f'Validation {wer_col} (%)')\n",
    "        ax.yaxis.set_major_formatter(PercentFormatter(xmax=1.0, decimals=0))\n",
    "        ax.grid(True, alpha=0.25)\n",
    "        ax.legend(frameon=False, loc='upper right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # textual summary\n",
    "    print(\"üìä Summary per Group\")\n",
    "    print(\"--------------------\")\n",
    "    for label in summary['label'].unique():\n",
    "        g = summary[summary['label'] == label]\n",
    "        m = g.loc[g['mean'].idxmin()]\n",
    "        best = float(m['mean'])\n",
    "        ep   = int(m[EPOCH_COL])\n",
    "        abs_impr = starting_wer - best\n",
    "        rel_impr = (abs_impr / starting_wer) * 100 if starting_wer else float('nan')\n",
    "        print(f\"üîé {label}: best {best:.4f} at epoch {ep} | Œî {abs_impr:.4f} ({rel_impr:.2f}%)\")\n",
    "\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e45a45",
   "metadata": {},
   "source": [
    "## Examples ‚Äî paste your job IDs and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d143b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A) From job IDs ‚Üí paths ‚Üí your plots & summaries\n",
    "\n",
    "# sanity: see what files a job id has\n",
    "debug_list_job(\"52354\")\n",
    "debug_list_job(\"52355\")\n",
    "\n",
    "# collect paths\n",
    "expA_paths = job_paths_from_ids(\"52354\", \"52355\")\n",
    "expB_paths = job_paths_from_ids(\"52356\", \"52357\")\n",
    "\n",
    "# show all imporant columns one run\n",
    "plot_result(expA_paths[0], title=\"Exp A ‚Äî single run\")\n",
    "\n",
    "# shows all runs of one experiment in one plot\n",
    "plot_pathological_wer_multi(\n",
    "    expA_paths,\n",
    "    title=\"Exp A ‚Äî per-run pathological WER\",           \n",
    "    )\n",
    "\n",
    "# best WER per file (pathological)\n",
    "lowest_pathological_wer(expA_paths)\n",
    "\n",
    "# healthy WER monotonic check\n",
    "healthy_never_rises_multi(expA_paths, tolerance=1e-4, verbose=True)\n",
    "\n",
    "# shows the mean WER over multiple runs of one experiment with mean ¬± 95% CI + summary of findings\n",
    "analyze_single_experiment(\n",
    "    expA_paths,\n",
    "    label=\"Exp A (w=1, aug=on)\",\n",
    "    max_epoch=30,\n",
    "    wer_col='collected_pathological_eval_val_wer',\n",
    "    show_baseline=True,\n",
    "    show_ci=True,\n",
    "    color='red'\n",
    ")\n",
    "\n",
    "# compares the WER across different experiments with mean ¬± 95% CI + summary of findings\n",
    "analyze_experiments_grouped(\n",
    "    {\"Exp A (w=1, aug=on)\": expA_paths,\n",
    "     \"Exp B (w=40, aug=off)\": expB_paths},\n",
    "    max_epoch=30,\n",
    "    wer_col='collected_pathological_eval_val_wer',\n",
    "    show_shadow=True,\n",
    "    color_map={\"Exp A (w=1, aug=on)\": \"red\",\n",
    "               \"Exp B (w=40, aug=off)\": \"blue\"}\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
